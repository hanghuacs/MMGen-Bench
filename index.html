<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="MMIG-Bench: Towards Comprehensive and Explainable Evaluation of Multi-Modal Image Generation Models">
  <meta name="keywords" content="Multi-Modal Image Generation, Benchmark, Compositionality, Evaluation, VQA">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MMIG-Bench: Towards Comprehensive and Explainable Evaluation of Multi-Modal Image Generation Models</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <!-- Use public CDNs for icon fonts -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/mmgen-bench.png">
  <style>
    .icon img {
      height: 1.1em;
      vertical-align: middle;
      margin-right: 2px;
      margin-bottom: 2px;
    }
    .link-block {
      margin-right: 0.5em;
      margin-bottom: 0.5em;
      display: inline-block;
    }
  </style>
</head>
<body>
<div class="navbar-menu">
  <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
    <div class="navbar-item has-dropdown is-hoverable">
      <p style="font-size:18px; display: inline; margin-right: -2px; margin-top: 12px;">ðŸ”¥</p>
      <a class="navbar-link">More Research</a>
      <div class="navbar-dropdown">
        <a class="navbar-item" href="https://hanghuacs.github.io/MMComposition/"><b>MMComposition</b></a>
        <a class="navbar-item" href="https://hanghuacs.github.io/finematch/"><b>FineMatch</b></a>
        <a class="navbar-item" href="https://yunlong10.github.io/VidComposition/"><b>VidComposition</b></a>
        <a class="navbar-item" href="https://hanghuacs.github.io/FineCaption/"><b>FineCaption</b></a>
      </div>
    </div>
  </div>
</div>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <img src="./static/images/mmgen-bench.png" width="50" style="vertical-align:middle;margin-bottom:8px;" />
            <span class="rainbow_text_animated">MMIG-Bench</span>: Towards Comprehensive and Explainable Evaluation of Multi-Modal Image Generation Models
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://hanghuacs.notion.site/">Hang Hua</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=b2DIlscAAAAJ">Ziyun Zeng</a> <sup>1</sup>,</span>
            <span class="author-block"><a href="https://song630.github.io/yizhisong.github.io/">Yizhi Song</a><sup>2</sup>,</span> 
            <span class="author-block"><a href="https://yunlong10.github.io/">Yunlong Tang</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://arking1995.github.io">Liu He</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://www.cs.purdue.edu/homes/aliaga/">Daniel Aliaga</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://wxiong.me/">Wei Xiong</a><sup>3</sup>,</span>
            <span class="author-block"><a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a><sup>1</sup></span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Rochester,</span>
            <span class="author-block"><sup>2</sup>Purdue University,</span>
            <span class="author-block"><sup>3</sup>NVIDIA</span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2411.15411"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2511.12345" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/hanghuacs/MMIG-Bench" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/datasets/hhua2/MMIG-Bench" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="HF"/>
                  </span>
                  <span>Data</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advances in multi-modal image generation models have led to remarkable progress in both text-to-image (T2I) and personalized generation tasks. However, existing benchmarks are fragmented, focusing on only a subset of capabilities or lacking compositional, explainable evaluation. To fill this gap, we propose <b style="color:#a6192e;">M</b>ulti-<b style="color:#a6192e;">M</b>odal <b style="color:#a6192e;">I</b>mage <b style="color:#a6192e;">G</b>eneration <b style="color:#a6192e;">Bench</b>mark (<b style="color:#a6192e;">MMIG-Bench</b>), a comprehensive benchmark for evaluating multi-modal image generation models. MMIG-Bench unifies compositional evaluation across T2I and customized generation, introduces explainable aspect-level metrics, and provides extensive human and automatic evaluations. Our results offer a thorough analysis of state-of-the-art diffusion, autoregressive, and API-based models, highlighting strengths and limitations and revealing future research directions for robust, explainable multi-modal generation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
  
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Contributions</h2>
    <div class="content has-text-justified">
      <ul>
        <li><b>Unified Benchmark:</b> MMIG-Bench offers the first unified, compositional benchmark for both T2I and personalized (customization) image generation models.</li>
        <li><b>Aspect-Level Explainability:</b> Proposes explainable aspect-level metrics (object, relation, attribute, counting) to evaluate fine-grained compositional capabilities.</li>
        <li><b>Comprehensive Evaluation:</b> Provides extensive comparisons with human studies and automated metrics across 18 state-of-the-art models.</li>
        <li><b>Open-Source Platform:</b> Publicly releases benchmark, code, datasets, and evaluation scripts to promote transparent and reproducible research.</li>
      </ul>
    </div>
  </div>
</section>
  <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <img src="./static/images/teaser.png" alt="Overview of MMIG-Bench" style="max-width:100%;border-radius:16px;box-shadow:0 2px 8px #eee;">
        <p class="is-size-5" style="margin-top:1em;">Overview of MMIG-Bench. We present a unified multi-modal benchmark which contains
1,750 multi-view reference images with 4,850 richly annotated text prompts, covering both text-only
and image-text-conditioned generation. We also propose a comprehensive three-level evaluation
framework: low-level of artifacts and identity preservation, mid-level of VQA-based Aspect Matching
Score, and high-level of aesthetics and human preferencesâ€”delivers holistic and interpretable scores.</p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Qualitative Examples of Different Models</h2>
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <img src="./static/images/figure4.png" alt="Qualitative examples of MMIG-Bench" style="max-width:100%;border-radius:16px;box-shadow:0 2px 8px #eee;">
        <p class="is-size-5" style="margin-top:1em;"> Representative qualitative results on MMIG-Bench. Our benchmark enables interpretable, compositional analysis of generation outputs at object, relation, attribute, and counting levels.</p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Comparison of Models (Text-to-Image)</h2>
        <div class="table-container">
          <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th>Method</th>
                <th>CLIP-T <span title="Higher is better">â†‘</span></th>
                <th>PAL4VST <span title="Lower is better">â†“</span></th>
                <th>AMS <span title="Higher is better">â†‘</span></th>
                <th>Human <span title="Higher is better">â†‘</span></th>
                <th>Aesthetic <span title="Higher is better">â†‘</span></th>
                <th>HPSv2 <span title="Higher is better">â†‘</span></th>
                <th>PickScore <span title="Higher is better">â†‘</span></th>
              </tr>
            </thead>
            <tbody>
              <tr style="background-color:#e9edf6;"><td colspan="8" style="text-align:center; font-weight:bold;"><i>Diffusion Models</i></td></tr>
              <tr>
                <td>SDXL</td>
                <td>33.529</td>
                <td>14.340</td>
                <td>79.08</td>
                <td>72.29</td>
                <td>6.337</td>
                <td>0.277</td>
                <td>0.120</td>
              </tr>
              <tr>
                <td>Photon-v1</td>
                <td>33.296</td>
                <td>2.947</td>
                <td>77.12</td>
                <td>69.49</td>
                <td><u>6.391</u></td>
                <td>0.284</td>
                <td>0.088</td>
              </tr>
              <tr>
                <td>Lumina-2</td>
                <td>33.281</td>
                <td>15.531</td>
                <td>84.11</td>
                <td>73.18</td>
                <td>6.048</td>
                <td>0.287</td>
                <td>0.116</td>
              </tr>
              <tr>
                <td>HunyuanDit-v1.2</td>
                <td>33.701</td>
                <td>8.024</td>
                <td>83.61</td>
                <td>74.89</td>
                <td>6.379</td>
                <td>0.300</td>
                <td>0.144</td>
              </tr>
              <tr>
                <td>Pixart-Sigma-xl2</td>
                <td>33.682</td>
                <td>9.283</td>
                <td>83.18</td>
                <td>76.65</td>
                <td>6.409</td>
                <td><u>0.304</u></td>
                <td><u>0.165</u></td>
              </tr>
              <tr>
                <td>Flux.1-dev</td>
                <td>33.017</td>
                <td>2.171</td>
                <td><u>84.44</u></td>
                <td><u>76.44</u></td>
                <td><b>6.433</b></td>
                <td><b>0.307</b></td>
                <td><b>0.210</b></td>
              </tr>
              <tr>
                <td>SD 3.5-large</td>
                <td><u>33.873</u></td>
                <td><u>6.359</u></td>
                <td>85.33</td>
                <td>77.04</td>
                <td>6.318</td>
                <td>0.294</td>
                <td>0.157</td>
              </tr>
              <tr>
                <td>HiDream-I1-Full</td>
                <td><b>33.876</b></td>
                <td><b>1.522</b></td>
                <td><b>89.65</b></td>
                <td><b>83.18</b></td>
                <td>6.457</td>
                <td>0.321</td>
                <td>0.450</td>
              </tr>
              <tr style="background-color:#e9edf6;"><td colspan="8" style="text-align:center; font-weight:bold;"><i>Autoregressive Models</i></td></tr>
              <tr>
                <td>JanusFlow</td>
                <td>31.498</td>
                <td>365.663</td>
                <td>70.25</td>
                <td>75.69</td>
                <td>5.221</td>
                <td>0.209</td>
                <td>0.031</td>
              </tr>
              <tr>
                <td>Janus-Pro-7B</td>
                <td>33.358</td>
                <td>31.954</td>
                <td>85.35</td>
                <td>80.36</td>
                <td>6.038</td>
                <td>0.275</td>
                <td>0.129</td>
              </tr>
              <tr style="background-color:#e9edf6;"><td colspan="8" style="text-align:center; font-weight:bold;"><i>API-based Models</i></td></tr>
              <tr>
                <td>Gemini-2.0-Flash</td>
                <td>32.433</td>
                <td>11.053</td>
                <td>85.35</td>
                <td>81.98</td>
                <td>6.102</td>
                <td>0.275</td>
                <td>0.110</td>
              </tr>
              <tr>
                <td>GPT-4o</td>
                <td>32.380</td>
                <td>3.497</td>
                <td>82.57</td>
                <td>81.02</td>
                <td><u>6.719</u></td>
                <td><u>0.279</u></td>
                <td><u>0.263</u></td>
              </tr>
            </tbody>
          </table>
          <p class="is-bold">Table 1. Quantitative comparison across 12 text-to-image models using 2,100 prompts. <b>Bold</b> indicates best in column; <u>Underline</u> second best.</p>
        </div>

        <h2 class="title is-3">Comparison of Multi-Modal Generation Models (Customization Task)</h2>
        <div class="table-container">
          <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th>Method</th>
                <th>CLIP-T <span title="Higher is better">â†‘</span></th>
                <th>CLIP-I <span title="Higher is better">â†‘</span></th>
                <th>DINOv2 <span title="Higher is better">â†‘</span></th>
                <th>CUTE <span title="Higher is better">â†‘</span></th>
                <th>PAL4VST <span title="Lower is better">â†“</span></th>
                <th>BLIPVQA <span title="Higher is better">â†‘</span></th>
                <th>AMS <span title="Higher is better">â†‘</span></th>
                <th>Aesthetic <span title="Higher is better">â†‘</span></th>
                <th>HPSv2 <span title="Higher is better">â†‘</span></th>
                <th>PickScore <span title="Higher is better">â†‘</span></th>
              </tr>
            </thead>
            <tbody>
              <tr style="background-color:#e9edf6;"><td colspan="11" style="text-align:center; font-weight:bold;"><i>Diffusion Models</i></td></tr>
              <tr>
                <td>BLIP Diffusion</td>
                <td>26.137</td>
                <td>80.286</td>
                <td>26.232</td>
                <td>69.681</td>
                <td>56.780</td>
                <td>0.247</td>
                <td>41.59</td>
                <td>5.830</td>
                <td>0.213</td>
                <td>0.032</td>
              </tr>
              <tr>
                <td>DreamBooth</td>
                <td>24.227</td>
                <td><b>88.758</b></td>
                <td><b>38.961</b></td>
                <td><b>79.780</b></td>
                <td>43.535</td>
                <td>0.108</td>
                <td>28.00</td>
                <td>5.368</td>
                <td>0.179</td>
                <td>0.019</td>
              </tr>
              <tr>
                <td>Emu2</td>
                <td><u>28.410</u></td>
                <td>79.026</td>
                <td><u>31.831</u></td>
                <td><u>71.132</u></td>
                <td>10.461</td>
                <td>0.378</td>
                <td><u>53.13</u></td>
                <td>5.639</td>
                <td><u>0.243</u></td>
                <td><u>0.066</u></td>
              </tr>
              <tr>
                <td>Ip-Adapter-XL</td>
                <td>28.577</td>
                <td><u>85.297</u></td>
                <td>34.177</td>
                <td>74.995</td>
                <td>8.531</td>
                <td>0.290</td>
                <td>51.10</td>
                <td><u>5.840</u></td>
                <td>0.233</td>
                <td>0.073</td>
              </tr>
              <tr>
                <td>MS Diffusion</td>
                <td>31.446</td>
                <td>77.827</td>
                <td>23.600</td>
                <td>71.306</td>
                <td><u>4.748</u></td>
                <td><u>0.496</u></td>
                <td>71.40</td>
                <td>5.979</td>
                <td>0.271</td>
                <td>0.143</td>
              </tr>
              <tr style="background-color:#e9edf6;"><td colspan="11" style="text-align:center; font-weight:bold;"><i>API-based Models</i></td></tr>
              <tr>
                <td>GPT-4o</td>
                <td><b>33.527</b></td>
                <td>75.152</td>
                <td>25.174</td>
                <td>64.776</td>
                <td><b>1.973</b></td>
                <td><b>0.672</b></td>
                <td><b>90.90</b></td>
                <td><b>6.368</b></td>
                <td><b>0.289</b></td>
                <td><b>0.550</b></td>
              </tr>
            </tbody>
          </table>
          <p class="is-bold">Table 2. Quantitative comparison across 6 multi-modal image generation models (1,690 samples). <b>Bold</b> indicates best in column; <u>Underline</u> second best.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://arxiv.org/pdf/2511.12345"><i class="fas fa-file-pdf"></i></a>
      <a class="icon-link" href="https://github.com/hanghuacs/MMIG-Bench" class="external-link"><i class="fab fa-github"></i></a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.</p>
          <p>You are welcome to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website.</p>
        </div>
      </div>
    </div>
  </div>
</footer>
</body>
</html>
