<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="FINECAPTION: Compositional Image Captioning Focusing on Wherever You Want at Any Granularity">
  <meta name="keywords" content="Aspect-based Image and Text Analysis, Compositionality">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FINECAPTION: Compositional Image Captioning Focusing on Wherever You Want at Any Granularity</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.webp">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">

  <div class="navbar-item has-dropdown is-hoverable">
        <p style="font-size:18px; display: inline; margin-right: -2px; margin-top: 12px;">ðŸ”¥</p>
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hanghuacs.github.io/MMComposition/">
            <b>MMComposition</b> 
        </a>
          <a class="navbar-item" href="https://hanghuacs.github.io/finematch/">
            <b>FineMatch</b> 
        </a>
          <a class="navbar-item" href="https://yunlong10.github.io/VidComposition/">
            <b>VidComposition</b> 
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><img src="./static/images/icon.webp" width="50" /><span class="rainbow_text_animated">FINECAPTION</span>: Compositional Image Captioning Focusing on Wherever You Want at Any Granularity</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://hanghuacs.notion.site/">Hang Hua</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://research.adobe.com/person/qing-liu/">Qing Liu</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://owenzlz.github.io">Lingzhi Zhang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://research.adobe.com/person/jing-shi/">Jing Shi</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://research.adobe.com/person/zhifei-zhang/">Zhifei Zhang</a><sup>1</sup>,
            </span>
            <br>
            <span class="author-block">
              <a href="https://yilinwang.org">Yilin Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://research.adobe.com/person/jianming-zhang/">Jianming Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://research.adobe.com/person/zhe-lin/">Zhe Lin</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Rochester,</span>
            <span class="author-block"><sup>2</sup>Adobe Research</span><br>
            <span class="author-block">
                      <h1 class="title is-4"><font color="#B03A2E"><b>CVPR 2025</b></font></h1>
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2411.15411"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2411.15411"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/hanghuacs/FineCaption"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/hhua2/finecaption"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          The advent of large Vision-Language Models (VLMs) has
significantly advanced multimodal tasks, enabling more sophisticated and accurate reasoning across various applications, including image and video captioning, visual question
answering, and cross-modal retrieval. Despite their superior capabilities, VLMs struggle with fine-grained image
regional composition information perception. Specifically,
they have difficulty accurately aligning the segmentation
masks with the corresponding semantics and precisely de-
scribing the compositional aspects of the referred regions.
However, compositionality â€“ the ability to understand and
generate novel combinations of known visual and textual
components â€“ is critical for facilitating coherent reasoning
and understanding across modalities by VLMs. To address
this issue, we propose FINECAPTION, a novel VLM that
can recognize arbitrary masks as referential inputs and pro-
cess high-resolution images for compositional image cap-
tioning at different granularity levels. To support this endeavor, we introduce COMPOSITIONCAP, a new dataset
for multi-grained region compositional image captioning,
which introduces the task of compositional attribute-aware
regional image captioning. Empirical results demonstrate the effectiveness of our proposed model compared to other
state-of-the-art VLMs. Additionally, we analyze the capabilities of current VLMs in recognizing various visual
prompts for compositional region image captioning, highlighting areas for improvement in VLM design and training.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Introduction</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Task Illustration</h3>
        <div class="content has-text-justified">
          <img src="./static/images/teaser-1.svg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          <p class="is-bold">
            We propose FINECAPTION, a novel Vision-Language model with the improved capabilities of Attribute-Aware Regional
Captioning, Regional Dense Captioning, and Comprehensive Global Image Captioning. FINECAPTION can recognize arbitrary masks
as referential inputs and process high-resolution images. Moreover, models trained using the traditional bounding boxes as region reference
are inadequate to precisely describe the region of interest.
          </p>
        </div>
        <h3 class="title is-4">Our Model</h3>
        <div class="content has-text-justified">
          <img src="./static/images/model.svg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          <p class="is-bold">
            Overview of FINECAPTION: The model incorporates a mask-aware visual encoder and two high-resolution encoders (ConvNext and SAM), enabling precise recognition of mask references and the perception of detailed compositional and spatial information for images.
          </p>
        </div>
 <section class="section" id="Case">
  <div class="content has-text-justified">
    <h2 class="title is-3">Performance</h2>
          <img src="./static/images/case.svg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          <p class="is-bold">
            FINECAPTION provides accurate and concise descriptions focused on specified attributes and regions, while GPT-4o often misses f ine-grained references and includes irrelevant information.
        </div>
</section>     

        <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Comparison of Models</h2>
        <div class="table-container">
          <style>
            table td, table th {
              white-space: nowrap;
            }
          </style>
          <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th rowspan="2">Model</th>
                <th colspan="3">Region Referral</th>
                <th colspan="5">Semantic Evaluation</th>
              </tr>
              <tr>
                <th>Visual Prompt</th>
                <th>Resolution</th>
                <th># Image Token</th>
                <th>ROUGE-L â†‘</th>
                <th>BLEU-4 â†‘</th>
                <th>METEOR â†‘</th>
                <th>CIDEr â†‘</th>
                <th>BERT Score â†‘</th>
              </tr>
            </thead>
            <tbody>
              <!-- Zero-shot learning -->
              <tr style="background-color:#e9edf6;">
                <td colspan="9" style="text-align:center; font-weight:bold;"><i>Zero-Shot Learning</i></td>
              </tr>
              <tr>
                <td>Kosmos-2</td><td>Bbox</td><td>224</td><td>256</td><td>9.21</td><td>0.14</td><td>1.98</td><td>1.07</td><td>37.69</td>
              </tr>
              <tr>
                <td>Alpha-CLIP-13B</td><td>Mask</td><td>336</td><td>576</td><td>13.89</td><td>0.51</td><td>5.94</td><td>2.68</td><td>42.01</td>
              </tr>
              <tr>
                <td>Qwen2-VL-7B</td><td>Bbox</td><td>AnyRes</td><td>-</td><td>14.12</td><td>0.57</td><td>6.18</td><td>2.74</td><td>42.97</td>
              </tr>
              <tr>
                <td>Ferret-13B</td><td>MContour</td><td>336</td><td>576</td><td>15.01</td><td>1.06</td><td>5.86</td><td>3.12</td><td>43.82</td>
              </tr>
              <tr>
                <td>ViP-LLaVA-13B</td><td>MContour</td><td>336</td><td>576</td><td>15.47</td><td>1.48</td><td>5.76</td><td>3.84</td><td>44.29</td>
              </tr>
              <tr>
                <td>LLaMA-3.2-11B-Vision-Instruction</td><td>Bbox</td><td>-</td><td>-</td><td>15.64</td><td>1.59</td><td>9.73</td><td>3.95</td><td>44.53</td>
              </tr>
              <tr>
                <td>LLaMA-3.2-90B-Vision-Instruction</td><td>Bbox</td><td>-</td><td>-</td><td>16.21</td><td>1.75</td><td>11.70</td><td>4.53</td><td>48.29</td>
              </tr>
              <tr>
                <td>InternVL-2-40B</td><td>Bbox</td><td>1792</td><td>4096</td><td>16.21</td><td>1.79</td><td>11.91</td><td>4.63</td><td>48.38</td>
              </tr>
              <tr style="background-color:#FFF9E3;">
                <td>GPT-4o</td><td>Bbox</td><td>-</td><td>-</td><td>17.87</td><td>3.21</td><td>12.87</td><td>6.49</td><td>49.85</td>
              </tr>
              <!-- Supervised learning -->
              <tr style="background-color:#e9edf6;">
                <td colspan="9" style="text-align:center; font-weight:bold;"><i>Supervised Learning</i></td>
              </tr>
              <tr>
                <td>Qwen2-VL-7B</td><td>Bbox</td><td>AnyRes</td><td>-</td><td>31.59</td><td>9.11</td><td>13.56</td><td>90.32</td><td>75.86</td>
              </tr>
              <tr>
                <td>LLaVA-1.6-13B</td><td>Bbox</td><td>AnyRes</td><td>576</td><td>31.72</td><td>9.35</td><td>13.64</td><td>90.71</td><td>75.89</td>
              </tr>
              <tr>
                <td>VILA1.5-8B</td><td>Bbox</td><td>336</td><td>144</td><td>31.87</td><td>9.03</td><td>13.79</td><td>90.01</td><td>75.95</td>
              </tr>
              <tr>
                <td>ViP-LLaVA-13B</td><td>MContour</td><td>336</td><td>576</td><td>32.42</td><td>9.97</td><td>14.82</td><td>91.44</td><td>76.77</td>
              </tr>
              <tr>
                <td>Alpha-CLIP-13B</td><td>Mask</td><td>336</td><td>576</td><td>35.68</td><td>10.96</td><td>16.11</td><td>93.85</td><td>77.66</td>
              </tr>
              <tr>
                <td>LLaVA-HR-X</td><td>Bbox</td><td>1024</td><td>1024</td><td>35.97</td><td>11.25</td><td>16.57</td><td>95.12</td><td>78.08</td>
              </tr>
              <tr>
                <td>LLaMA-3.2-11B-Vision</td><td>Bbox</td><td>-</td><td>-</td><td>38.14</td><td>12.87</td><td>18.31</td><td>99.11</td><td>78.94</td>
              </tr>
              <tr style="background-color:#DAE8FC;">
                <td><b>FINECAPTION-8B (ours)</b></td><td>Mask</td><td>1024</td><td>1024</td>
                <td><b>41.05</b></td><td><b>14.46</b></td><td><b>22.01</b></td><td><b>127.95</b></td><td><b>80.97</b></td>
              </tr>
            </tbody>
          </table>
          <p class="is-bold">Table 2. Comparison of the capabilities of FINECAPTION and other related VLMs including both open-sourced models and API-based models.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
  @article{hua2024finecaption,
  title={FINECAPTION: Compositional Image Captioning Focusing on Wherever You Want at Any Granularity},
  author={Hua, Hang and Liu, Qing and Zhang, Lingzhi and Shi, Jing and Zhang, Zhifei and Wang, Yilin and Zhang, Jianming and Luo, Jiebo},
  journal={arXiv preprint arXiv:2411.15411},
  year={2024}
}
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
